{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model - Bahdanau Attention for Text with Glove.ipynb","provenance":[{"file_id":"1HciI15--bFHvVfB_yyuCPDI7ye-yhLdc","timestamp":1607622158762},{"file_id":"1420y6yya2hoPYbQvPoXf5ohjPUP7MqXE","timestamp":1607364814185},{"file_id":"12mqxeZGpP2lyzIh16ARJOqN4UX5Suxxp","timestamp":1607358661548},{"file_id":"1lThUcBGDtrqnW2fb_phm4ZLWNaDDF3s-","timestamp":1605739998273},{"file_id":"1ywwNBd9MjP1sKw0b6wmFkUnvO-sL0Dp9","timestamp":1604772341530}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"phtTRHEDQBqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607629545643,"user_tz":360,"elapsed":18054,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}},"outputId":"abba36a8-86fa-477b-e12d-30152214dadc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Zdpgg6tLiGr","executionInfo":{"status":"ok","timestamp":1607629567170,"user_tz":360,"elapsed":420,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["import numpy as np\n","from numpy import array\n","from pickle import load, dump\n","import keras\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcjMdsovv61I","executionInfo":{"status":"ok","timestamp":1607629553220,"user_tz":360,"elapsed":2753,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["import sys\r\n","import os\r\n","\r\n","# Helper functions to load previously processed image features, cleaned image descriptions, and other saved items\r\n","py_file_location = '/content/drive/MyDrive/ImageCaptioningProject/Notebooks/FinalizedNotebooks'\r\n","sys.path.append(os.path.abspath(py_file_location))\r\n","\r\n","import HelperFunctions as helper"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKZXkYxwdASX"},"source":["## GloVe embeddings"]},{"cell_type":"code","metadata":{"id":"u3xTV12dO9Sp","executionInfo":{"status":"ok","timestamp":1607629569941,"user_tz":360,"elapsed":568,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["def getDicts(descriptions, word_count_threshold=None):\n","  all_captions = helper.toLines(descriptions)\n","  word_counts = {}\n","  for sent in all_captions:\n","      for w in sent.split(' '):\n","          word_counts[w] = word_counts.get(w, 0) + 1\n","  vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","  # Create two dictionaries: one to map word to an index, one to map index to a word\n","  # Also adding 1 to length of vocabulary, appending 0s at end of all captions to make them equal length\n","  ixtoword = {}\n","  wordtoix = {}\n","  ix = 1\n","  for w in vocab:\n","      wordtoix[w] = ix\n","      ixtoword[ix] = w\n","      ix += 1\n","  vocab_size = len(ixtoword) + 1\n","  return vocab_size, wordtoix, ixtoword\n","\n","def getGloveEmbeddings(wordtoix, vocab_size):\n","  embeddings_index = {}\n","  # File location of GloVe embeddings\n","  glove_file = open('/content/drive/MyDrive/ImageCaptioningProject/glove.6B.200d.txt', encoding='utf-8')\n","  for line in glove_file:\n","      values = line.split()\n","      word = values[0]\n","      coefs = np.asarray(values[1:], dtype='float32')\n","      embeddings_index[word] = coefs\n","  glove_file.close()\n","  # Make the matrix of shape ({vocab length},200) consisting of our vocabulary and the 200-d vector.\n","  embedding_dim = 200\n","  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","  for word, i in wordtoix.items():\n","      embedding_vector = embeddings_index.get(word)\n","      if embedding_vector is not None:\n","          embedding_matrix[i] = embedding_vector\n","  return embedding_matrix"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNsRF_pxMh-o"},"source":["## Creating sequences\n","\n","Takes the word-to-index dictionary, the index-to-word dictionary, the maximum length of the descriptions, dictionary of all descriptions, and dictionary of photo features and transforms data into input/output pairs of data for training the model."]},{"cell_type":"code","metadata":{"id":"yvyLIMNJ_4OG","executionInfo":{"status":"ok","timestamp":1607629572066,"user_tz":360,"elapsed":785,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# Create sequences of images, input sequences, and output words for an image\n","def createSequences(wordtoix, max_length, descriptions, photos_features, vocab_size):\n","  X1, X2, y = list(), list(), list()\n","  # Walk through each image id\n","  for key, desc_list in descriptions.items():\n","    # Walk through each description for the image\n","    for desc in desc_list:\n","      # Encode the sequence\n","      seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n","      # split one sequence into multiple X,y pairs\n","      for i in range(1, len(seq)):\n","        # split into input and output pair - words up to i, and i (where i is the next word)\n","        in_seq, out_seq, = seq[:i], seq[i]\n","        # pad input sequence \n","        in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n","        # encode output sequence\n","        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","        # store\n","        X1.append(photos_features[key][0])\n","        X2.append(in_seq)\n","        y.append(out_seq)\n","  return array(X1), array(X2), array(y)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oaM85V-bpytE"},"source":["## Load training data \n","Creates input/output pairs for the training data <br>\n","input: image features, text descriptions <br>\n","output: next word"]},{"cell_type":"code","metadata":{"id":"8zQ2Dpx7oLGV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607629703079,"user_tz":360,"elapsed":36909,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}},"outputId":"1344ffc9-897d-41d7-ad0b-f662123a5de5"},"source":["descriptions_file = '/content/drive/My Drive/ImageCaptioningProject/descriptions.txt'\n","features_file = '/content/drive/My Drive/ImageCaptioningProject/features.pkl'\n","\n","# Load training dataset\n","train_image_ids_file = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.trainImages.txt'\n","train_ids = helper.loadImageIds(train_image_ids_file)\n","train_descriptions = helper.loadCleanDescriptions(descriptions_file, train_ids)\n","train_features = helper.loadImageFeatures(features_file, train_ids)\n","\n","# Limit to words that have been used at least n times\n","vocab_size, wordtoix, ixtoword = getDicts(train_descriptions, word_count_threshold=4)\n","# Save wordtoix and ixtoword to be used to generate new captions\n","dump(wordtoix, open('/content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/wordtoix.pkl', 'wb'))\n","dump(ixtoword, open('/content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/ixtoword.pkl', 'wb'))\n","print('Vocab size: %d' % vocab_size)\n","\n","# Get GloVe embedding matrix of descriptions\n","glove_embeddings_matrix = getGloveEmbeddings(wordtoix=wordtoix, vocab_size=vocab_size)\n","# Determine the maximum sequence (i.e. words in description) length\n","max_length = helper.calcMaxLength(train_descriptions)\n","print('Max length: %d' % max_length)\n","\n","# Prepare training sequences\n","X1train, X2train, ytrain = createSequences(wordtoix, max_length, train_descriptions, train_features, vocab_size)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Vocab size: 2915\n","Max length: 34\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ezIehr6yNu4Y"},"source":["## Load validation data\n","Creates input/output pairs for the validation data"]},{"cell_type":"code","metadata":{"id":"hL-h8sfDrYsr","executionInfo":{"status":"ok","timestamp":1607629739081,"user_tz":360,"elapsed":2918,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# load validation dataset\n","val_image_ids_file = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.devImages.txt'\n","val_ids = helper.loadImageIds(val_image_ids_file)\n","val_descriptions = helper.loadCleanDescriptions(descriptions_file, val_ids)\n","val_features = helper.loadImageFeatures(features_file, val_ids)\n","\n","# prepare sequences\n","X1val, X2val, yval = createSequences(wordtoix, max_length, val_descriptions, val_features, vocab_size)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iynH1H5fGhCU"},"source":["### Bahdanau Attention"]},{"cell_type":"code","metadata":{"id":"IOGUq5PKGgbM","executionInfo":{"status":"ok","timestamp":1607629756327,"user_tz":360,"elapsed":426,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["from keras.layers import Layer\r\n","import keras.backend as K\r\n","\r\n","class attention(Layer):\r\n","    def __init__(self,**kwargs):\r\n","        super(attention,self).__init__(**kwargs)\r\n","\r\n","    def build(self,input_shape):\r\n","        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\r\n","        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \r\n","        super(attention, self).build(input_shape)\r\n","\r\n","    def call(self,x):\r\n","        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\r\n","        at=K.softmax(et)\r\n","        at=K.expand_dims(at,axis=-1)\r\n","        output=x*at\r\n","        return K.sum(output,axis=1)\r\n","\r\n","    def compute_output_shape(self,input_shape):\r\n","        return (input_shape[0],input_shape[-1])\r\n","\r\n","    def get_config(self):\r\n","        return super(attention,self).get_config()"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvTp7sFZLpWH"},"source":["## Bulding model\n"]},{"cell_type":"code","metadata":{"id":"R4zJlXBbA0jY","executionInfo":{"status":"ok","timestamp":1607629906790,"user_tz":360,"elapsed":514,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# https://github.com/keras-team/keras/issues/4962\n","\n","from keras.layers import Flatten, Activation, RepeatVector, Permute\n","\n","# Defining the captioning model\n","def getModel(vocab_size, max_length):\n","  # feature extractor model\n","  inputs1 = Input(shape=(2048,))\n","  fe1 = Dropout(0.5)(inputs1)\n","  fe2 = Dense(256, activation='relu')(fe1)\n","  # sequence model\n","  inputs2 = Input(shape=(max_length,))\n","  se1 = Embedding(vocab_size, 200, mask_zero=True, name='se1')(inputs2)\n","  se2 = Dropout(0.5)(se1)\n","  text_att_in = LSTM(256, return_sequences=True)(se2)\n","  text_att_out = attention()(text_att_in)\n","  # decoder model\n","  decoder1 = add([fe2, text_att_out])\n","  decoder2 = Dense(256, activation='relu')(decoder1)\n","  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","  # tie it together [image, seq] [word]\n","  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\n","  # do not want to retrain the weights in text embedding layer (pre-trained Glove vectors)\n","  model.get_layer('se1').set_weights([glove_embeddings_matrix])\n","  model.get_layer('se1').trainable = False\n","  model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","  # summarize model\n","  print(model.summary())\n","  plot_model(model, to_file='/content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/Text_model.png', show_shapes=True)\n","  return model"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZ4v2QJfN2nM"},"source":["## Train model\n","Models with improved loss are saved each epoch"]},{"cell_type":"code","metadata":{"id":"k-7ckCgxmYwE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607636944482,"user_tz":360,"elapsed":7036062,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}},"outputId":"880fb927-d827-4752-9f4b-f74fdd33d69e"},"source":["model = getModel(vocab_size, max_length)\n","\n","# Adding checkpoint - save the model when it improves, \n","# and then use the model with the best skill as the final model.\n","# https://www.tensorflow.org/tutorials/keras/save_and_load\n","# SavedModel_format rather than .h5, since saving a custom model\n","\n","# Define checkpoint callback\n","filepath = '/content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}'\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","# fit model\n","history = model.fit([X1train, X2train], ytrain, epochs=10, verbose=2, callbacks=[checkpoint], validation_data=([X1val, X2val], yval))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Model: \"functional_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            [(None, 34)]         0                                            \n","__________________________________________________________________________________________________\n","se1 (Embedding)                 (None, 34, 200)      583000      input_4[0][0]                    \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 2048)]       0                                            \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 34, 200)      0           se1[0][0]                        \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 34, 256)      467968      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","attention_1 (attention)         (None, 256)          290         lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n","                                                                 attention_1[0][0]                \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 2915)         749155      dense_4[0][0]                    \n","==================================================================================================\n","Total params: 2,390,749\n","Trainable params: 1,807,749\n","Non-trainable params: 583,000\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/10\n","\n","Epoch 00001: val_loss improved from inf to 4.04794, saving model to /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep001-loss4.622-val_loss4.048\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep001-loss4.622-val_loss4.048/assets\n","9364/9364 - 740s - loss: 4.6217 - val_loss: 4.0479\n","Epoch 2/10\n","\n","Epoch 00002: val_loss improved from 4.04794 to 3.75995, saving model to /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep002-loss3.843-val_loss3.760\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep002-loss3.843-val_loss3.760/assets\n","9364/9364 - 740s - loss: 3.8432 - val_loss: 3.7600\n","Epoch 3/10\n","\n","Epoch 00003: val_loss improved from 3.75995 to 3.66490, saving model to /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep003-loss3.596-val_loss3.665\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep003-loss3.596-val_loss3.665/assets\n","9364/9364 - 739s - loss: 3.5962 - val_loss: 3.6649\n","Epoch 4/10\n","\n","Epoch 00004: val_loss improved from 3.66490 to 3.63411, saving model to /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep004-loss3.476-val_loss3.634\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep004-loss3.476-val_loss3.634/assets\n","9364/9364 - 734s - loss: 3.4760 - val_loss: 3.6341\n","Epoch 5/10\n","\n","Epoch 00005: val_loss improved from 3.63411 to 3.63394, saving model to /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep005-loss3.404-val_loss3.634\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep005-loss3.404-val_loss3.634/assets\n","9364/9364 - 720s - loss: 3.4043 - val_loss: 3.6339\n","Epoch 6/10\n","\n","Epoch 00006: val_loss improved from 3.63394 to 3.61178, saving model to /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep006-loss3.360-val_loss3.612\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/model-ep006-loss3.360-val_loss3.612/assets\n","9364/9364 - 712s - loss: 3.3597 - val_loss: 3.6118\n","Epoch 7/10\n","\n","Epoch 00007: val_loss did not improve from 3.61178\n","9364/9364 - 685s - loss: 3.3288 - val_loss: 3.6201\n","Epoch 8/10\n","\n","Epoch 00008: val_loss did not improve from 3.61178\n","9364/9364 - 669s - loss: 3.3074 - val_loss: 3.6499\n","Epoch 9/10\n","\n","Epoch 00009: val_loss did not improve from 3.61178\n","9364/9364 - 648s - loss: 3.2943 - val_loss: 3.6540\n","Epoch 10/10\n","\n","Epoch 00010: val_loss did not improve from 3.61178\n","9364/9364 - 642s - loss: 3.2810 - val_loss: 3.6729\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DFthHfU54tYN","executionInfo":{"status":"ok","timestamp":1607637135173,"user_tz":360,"elapsed":459,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# Save model history\r\n","dump(history.history, open('/content/drive/MyDrive/ImageCaptioningProject/FinalizedModels/TextModels/history', 'wb'))"],"execution_count":19,"outputs":[]}]}