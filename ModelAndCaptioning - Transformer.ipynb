{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ModelAndCaptioning - Transformer.ipynb","provenance":[{"file_id":"1lThUcBGDtrqnW2fb_phm4ZLWNaDDF3s-","timestamp":1605739998273},{"file_id":"1ywwNBd9MjP1sKw0b6wmFkUnvO-sL0Dp9","timestamp":1604772341530}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phtTRHEDQBqf","executionInfo":{"status":"ok","timestamp":1605831071811,"user_tz":360,"elapsed":377,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}},"outputId":"cf505c3d-5474-430c-c368-4d6652f37fef"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Zdpgg6tLiGr","executionInfo":{"status":"ok","timestamp":1605831173404,"user_tz":360,"elapsed":367,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["from numpy import array\n","from pickle import load\n","import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CeGd3dQTMI1j"},"source":["## File Loading Helper Functions\n","Helps load previously processed image features and cleaned image descriptions."]},{"cell_type":"code","metadata":{"id":"pzhn11C_UNDU","executionInfo":{"status":"ok","timestamp":1605831080229,"user_tz":360,"elapsed":389,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# load doc into memory\n","def load_doc(filename):\n","  # open the file as read only\n","  file = open(filename, 'r')\n","  # read all text\n","  text = file.read()\n","  # close the file\n","  file.close()\n","  return text\n","\n","# load list of unique photo ids, derived from image file names\n","def load_set(filename):\n","  doc = load_doc(filename)\n","  dataset = list()\n","  # process line by line\n","  for line in doc.split('\\n'):\n","    # skip empty lines\n","    if len(line) < 1:\n","      continue\n","    # get the image identifier\n","    identifier = line.split('.')[0]\n","    dataset.append(identifier)\n","  return set(dataset)\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","    # load document\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","      # split line by white space\n","      tokens = line.split()\n","      # split id from description\n","      image_id, image_desc = tokens[0], tokens[1:]\n","      # skip images not in the set\n","      if image_id in dataset:\n","        # create list\n","        if image_id not in descriptions:\n","          descriptions[image_id] = list()\n","        # wrap description in tokens\n","        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","        # store\n","        descriptions[image_id].append(desc)\n","    return descriptions\n","\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EW_C1XnkL16J"},"source":["## Tokenizer and Tokenizer helper functions\n","Tokenizer encodes English captions to vectors,\n","and transforms those vectors into uniform-length sequences. Tokenizer is fitted upon the training descriptions text. "]},{"cell_type":"code","metadata":{"id":"7jsirLxko2cx","executionInfo":{"status":"ok","timestamp":1605831082027,"user_tz":360,"elapsed":362,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# Build tokenizer\n","# Note: add limit to vocabulay?  ~9500 to 5000\n","\n","# convert a dictionary of clean descriptions (image_id: list of descriptions) to a general list of all descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n"," \n","#vocab_limit = 5000\n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\t#tokenizer = Tokenizer(num_words=vocab_limit)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","# Helper function; calculate the length of the description with the most words\n","def calc_max_length(description):\n","  lines = to_lines(description)\n","  return max(len(d.split()) for d in lines)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNsRF_pxMh-o"},"source":["## Creating sequences\n","\n","Takes the tokenizer, the maximum length of the descriptions, dictionary of all descriptions, and dictionary of photo features and transforms data into input/output pairs of data for training the model."]},{"cell_type":"code","metadata":{"id":"yvyLIMNJ_4OG","executionInfo":{"status":"ok","timestamp":1605831083634,"user_tz":360,"elapsed":404,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# create sequences of images, input sequences, and output words for an image\n","def create_sequences(tokenizer, max_length, descriptions, photos_features, vocab_size):\n","  X1, X2, y = list(), list(), list()\n","  # walk through each image id\n","  for key, desc_list in descriptions.items():\n","    # walk through each description for the image\n","    for desc in desc_list:\n","      # encode the sequence\n","      seq = tokenizer.texts_to_sequences([desc])[0]\n","      # split one sequence into multiple X,y pairs\n","      for i in range(1, len(seq)):\n","        # split into input and output pair - words up to i, and i (where i is the next word)\n","        in_seq, out_seq, = seq[:i], seq[i]\n","        # pad input sequence \n","        in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n","        # encode output sequence\n","        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","        # store\n","        X1.append(photos_features[key][0])\n","        X2.append(in_seq)\n","        y.append(out_seq)\n","  return array(X1), array(X2), array(y)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLJ_0DoQLvfY"},"source":["## Transformer\n","Implements attention for the text descriptons."]},{"cell_type":"code","metadata":{"id":"4s7z0ipl6KGl","executionInfo":{"status":"ok","timestamp":1605831100708,"user_tz":360,"elapsed":371,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# https://keras.io/examples/nlp/text_classification_with_transformer/\n","\n","import tensorflow as tf\n","from keras import layers\n","\n","# Implement multi head self attention as a Keras layer\n","class MultiHeadSelfAttention(layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\n","                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n","            )\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = layers.Dense(embed_dim)\n","        self.key_dense = layers.Dense(embed_dim)\n","        self.value_dense = layers.Dense(embed_dim)\n","        self.combine_heads = layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        # x.shape = [batch_size, seq_len, embedding_dim]\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        query = self.separate_heads(\n","            query, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        key = self.separate_heads(\n","            key, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        value = self.separate_heads(\n","            value, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        attention, weights = self.attention(query, key, value)\n","        attention = tf.transpose(\n","            attention, perm=[0, 2, 1, 3]\n","        )  # (batch_size, seq_len, num_heads, projection_dim)\n","        concat_attention = tf.reshape(\n","            attention, (batch_size, -1, self.embed_dim)\n","        )  # (batch_size, seq_len, embed_dim)\n","        output = self.combine_heads(\n","            concat_attention\n","        )  # (batch_size, seq_len, embed_dim)\n","        return output\n","    \n","    def get_config(self):\n","        config = super(MultiHeadSelfAttention, self).get_config()\n","        config.update({\"embed_dim\": self.embed_dim,\n","                \"num_heads\": self.num_heads,\n","                \"projection_dim\": self.projection_dim,\n","                \"query_dense\": self.query_dense,\n","                \"key_dense\": self.key_dense,\n","                \"value_dense\": self.value_dense,\n","                \"combine_heads\": self.combine_heads})\n","        return config   \n","\n","\n","# Implement a Transformer block as a layer\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","    \n","    def get_config(self):\n","        config = super(TransformerBlock, self).get_config()\n","        config.update({\"att\": self.att,\n","                \"ffn\": self.ffn,\n","                \"layernorm1\": self.layernorm1,\n","                \"layernorm2\": self.layernorm2,\n","                \"dropout1\": self.dropout1,\n","                \"dropout2\": self.dropout2\n","                })\n","        return config\n","        #config = super(TransformerBlock)\n","        #return cfg   \n","\n","# Implement embedding layer (Do we need this? Are we to keep current embedding layer for model? - I think DELETE)\n","# Two seperate embedding layers: one for tokens, one for token index (position)\n","class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n","\n","    # https://stackoverflow.com/questions/58678836/notimplementederror-layers-with-arguments-in-init-must-override-get-conf\n","    def get_config(self):\n","        config = super(TokenAndPositionEmbedding, self).get_config()\n","        config.update({\"token_emb\": self.token_emb,\n","                \"pos_emb\": self.pos_emb})\n","        return config\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvTp7sFZLpWH"},"source":["## Bulding model with transformer for text"]},{"cell_type":"code","metadata":{"id":"BCOjuI0zhwR-","executionInfo":{"status":"ok","timestamp":1605832917170,"user_tz":360,"elapsed":443,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# Defining the captioning model\n","# Two inputs: photo (Photo Feature Extractor), and word sequences (Sequence Processor)\n","# Regularlization: 50% dropout to avoid overfitting\n","# Output of model: next word in sequence\n","# NOTE: InceptionV3 -> 2048 feature vector\n","\n","from keras.layers import GlobalAveragePooling1D\n","\n","def define_model(vocab_size, max_length):\n","  # feature extractor model. Input: photo features vector of 2048 elements\n","  inputs1 = Input(shape=(2048,))\n","  fe1 = Dropout(0.5)(inputs1)\n","  # Use convolution to reduce features vector from 2048 to 256\n","  fe2 = Dense(256, activation='relu')(fe1)\n","  # sequence model\n","  inputs2 = Input(shape=(max_length,))\n","  # uses mask to ignore padded values\n","  se1 = TokenAndPositionEmbedding(max_length, vocab_size, 256)(inputs2) \n","  se2 = TransformerBlock(256, 2, 256)(se1) # where TransformerBlock(embed_dim, num_heads, ff_dim)\n","  se3 = GlobalAveragePooling1D()(se2) \n","  se4 = Dropout(0.5)(se3)\n","  se5 = Dense(256, activation='relu')(se4)\n","  se6 = Dropout(0.25)(se5)\n","  # decoder model\n","  decoder1 = add([fe2, se6])\n","  decoder2 = Dense(256, activation='relu')(decoder1)\n","  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","  # tie it together [image, seq] -> [word]\n","  model = Model(inputs=[inputs1, inputs2], outputs = outputs)\n","  optimizer = keras.optimizers.Adam(lr=0.01)\n","  model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","  # save model layout \n","  print(model.summary())\n","  plot_model(model, to_file='/content/drive/MyDrive/ImageCaptioningProject/NewModels/transformer_model.png', show_shapes=True)\n","  return model"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oaM85V-bpytE"},"source":["## Load training data \n","Creates input/output pairs for the training data <br>\n","input: image features, text descriptions <br>\n","output: next word"]},{"cell_type":"code","metadata":{"id":"8zQ2Dpx7oLGV","executionInfo":{"status":"ok","timestamp":1605831134353,"user_tz":360,"elapsed":24667,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# load training dataset\n","filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', train)\n","# photo features\n","train_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', train)\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","\n","# Embedding layer expects input_dim to be vocab size + 1\n","vocab_size = len(tokenizer.word_index) + 1\n","# determine the maximum sequence length\n","max_length = calc_max_length(train_descriptions)\n","# prepare sequences\n","X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezIehr6yNu4Y"},"source":["## Load validation data\n","Creates input/output pairs for the validation data"]},{"cell_type":"code","metadata":{"id":"hL-h8sfDrYsr","executionInfo":{"status":"ok","timestamp":1605831138145,"user_tz":360,"elapsed":24517,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["# load validation set\n","filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.devImages.txt'\n","validation = load_set(filename)\n","# descriptions\n","validation_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', validation)\n","# photo features\n","validation_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', validation)\n","\n","# prepare sequences\n","X1val, X2val, yval = create_sequences(tokenizer, max_length, validation_descriptions, validation_features, vocab_size)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZ4v2QJfN2nM"},"source":["## Train model\n","Models with improved loss are saved each epoch"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-7ckCgxmYwE","outputId":"e1953f1c-1b7a-4b70-f412-34a04af76979"},"source":["# Fit model\n","\n","# define the model\n","model = define_model(vocab_size, max_length)\n","\n","# Adding checkpoint - save the model when it improves, \n","# and then use the model with the best skill as the final model.\n","# https://www.tensorflow.org/tutorials/keras/save_and_load\n","# SavedModel_format rather than .h5, since saving a custom model\n","\n","# Define checkpoint callback\n","filepath = '/content/drive/MyDrive/ImageCaptioningProject/NewModels/TransformerModelsWithLR/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}'\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","# fit model\n","model.fit([X1train, X2train], ytrain, epochs=10, verbose=1, callbacks=[checkpoint], validation_data=([X1val, X2val], yval))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_11\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_11 (InputLayer)           [(None, 34)]         0                                            \n","__________________________________________________________________________________________________\n","token_and_position_embedding_3  (None, 34, 256)      1948928     input_11[0][0]                   \n","__________________________________________________________________________________________________\n","transformer_block_3 (Transforme (None, 34, 256)      395776      token_and_position_embedding_3[0]\n","__________________________________________________________________________________________________\n","global_average_pooling1d_2 (Glo (None, 256)          0           transformer_block_3[0][0]        \n","__________________________________________________________________________________________________\n","input_10 (InputLayer)           [(None, 2048)]       0                                            \n","__________________________________________________________________________________________________\n","dropout_12 (Dropout)            (None, 256)          0           global_average_pooling1d_2[0][0] \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 2048)         0           input_10[0][0]                   \n","__________________________________________________________________________________________________\n","dense_32 (Dense)                (None, 256)          65792       dropout_12[0][0]                 \n","__________________________________________________________________________________________________\n","dense_25 (Dense)                (None, 256)          524544      dropout_9[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_13 (Dropout)            (None, 256)          0           dense_32[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 256)          0           dense_25[0][0]                   \n","                                                                 dropout_13[0][0]                 \n","__________________________________________________________________________________________________\n","dense_33 (Dense)                (None, 256)          65792       add_2[0][0]                      \n","__________________________________________________________________________________________________\n","dense_34 (Dense)                (None, 7579)         1947803     dense_33[0][0]                   \n","==================================================================================================\n","Total params: 4,948,635\n","Trainable params: 4,948,635\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/10\n","8098/9576 [========================>.....] - ETA: 27s - loss: 5.8336"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IZE436TEOEaB"},"source":["## Evaluate model"]},{"cell_type":"code","metadata":{"id":"ene71bgK43GV","executionInfo":{"status":"ok","timestamp":1605832442252,"user_tz":360,"elapsed":1541,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["from nltk.translate.bleu_score import corpus_bleu\n","\n","# map an integer to word\n","def word_for_id(integer, tokenizer):\n","  for word, index in tokenizer.word_index.items():\n","    if index == integer:\n","      return word\n","  return none\n","\n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","  # seed generation process with start flag\n","  in_text = 'startseq'\n","  # iterate over the whole length of the sequence\n","  for i in range(max_length):\n","    # integer encode input sequence \n","    sequence = tokenizer.texts_to_sequences([in_text])[0]\n","    # pad input\n","    sequence = pad_sequences([sequence], maxlen=max_length)\n","    # predict next word\n","    yhat = model.predict([photo, sequence], verbose=0)\n","    # convert probability to an integer\n","    yhat = argmax(yhat)\n","    # map integer to word\n","    word = word_for_id(yhat, tokenizer)\n","    # stop if we cannot map the word\n","    if word is None:\n","      break\n","    # append as input for generating the next word\n","    in_text += ' ' + word\n","    # stop if we predict the end of the sequence \n","    if word == 'endseq':\n","      break\n","  return in_text\n","\n","# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","  actual, predicted = list(), list()\n","  # step over the whole set\n","  for key, desc_list in descriptions.items():\n","    # generate descriptions\n","    yhat = generate_desc(model, tokenizer, photos[key], max_length)\n","    # store actual and predicted\n","    references = [d.split() for d in desc_list]\n","    actual.append(references)\n","    predicted.append(yhat.split())\n","  # calculate BLEU scores\n","  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5FMEWu7qKfO"},"source":["from keras.models import load_model\n","from numpy import argmax\n","\n","# load tokenizer\n","tokenizer = load(open('/content/drive/My Drive/ImageCaptioningProject/develop_tokenizer.pkl', 'rb'))\n","# previously defined\n","max_length = 34\n","\n","# load test set, unseen by the model\n","filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.testImages.txt'\n","test = load_set(filename)\n","# descriptions\n","test_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', test)\n","# photo features\n","test_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', test)\n","\n","# load the model\n","filename = '/content/drive/MyDrive/ImageCaptioningProject/NewModels/TransformerModels/latestModel/'\n","saved_model = load_model(filename)\n","\n","# evaluate model\n","evaluate_model(saved_model, test_descriptions, test_features, tokenizer, max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpJGoiTAIrsP","executionInfo":{"status":"ok","timestamp":1605832526683,"user_tz":360,"elapsed":825,"user":{"displayName":"Shelby Mohar","photoUrl":"","userId":"10900162504420920907"}}},"source":["from tensorflow.keras.applications import InceptionV3\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.inception_v3 import preprocess_input\n","\n","# extract features of new uncaptioned photos\n","def extract_features(filename):\n","  # load model\n","  features_model = InceptionV3()\n","  # omitting unneccessary classification layer\n","  features_model = Model(inputs=features_model.inputs, outputs=features_model.layers[-2].output)\n","  # load photo\n","  image = load_img(filename, target_size=(299, 299))\n","  # convert image pixels to numpy array\n","  image = img_to_array(image)\n","  # reshape image array for model input\n","  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) \n","  # prepare image for CNN. Normalizes image array to range [-1, 1], matching format of images used to train InceptionV3\n","  image = preprocess_input(image)\n","  # get image features\n","  feature = features_model.predict(image, verbose=0)\n","  return feature"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"6544tPRHO0Mn"},"source":["from keras.models import load_model\n","from numpy import argmax\n","saved_model = load_model('/content/drive/MyDrive/ImageCaptioningProject/NewModels/TransformerModelsWithLR/model-ep002-loss5.782-val_loss5.686')\n","\n","photo = extract_features('/content/drive/MyDrive/ImageCaptioningProject/uncaptioned_images/example3.jpg')\n","description = generate_desc(saved_model, tokenizer, photo, max_length)\n","print(description)"],"execution_count":null,"outputs":[]}]}