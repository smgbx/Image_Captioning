{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelAndCaptioning - Adding Bahdanau Attention to Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMXtZmkNaMoP"
      },
      "source": [
        "https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phtTRHEDQBqf",
        "outputId": "e833329a-43d4-41c7-ed96-a03821574ad1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zdpgg6tLiGr"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeGd3dQTMI1j"
      },
      "source": [
        "## File Loading Helper Functions\n",
        "Helps load previously processed image features and cleaned image descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzhn11C_UNDU"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load list of unique photo ids, derived from image file names\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "    # skip empty lines\n",
        "    if len(line) < 1:\n",
        "      continue\n",
        "    # get the image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    # load document\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "      # split line by white space\n",
        "      tokens = line.split()\n",
        "      # split id from description\n",
        "      image_id, image_desc = tokens[0], tokens[1:]\n",
        "      # skip images not in the set\n",
        "      if image_id in dataset:\n",
        "        # create list\n",
        "        if image_id not in descriptions:\n",
        "          descriptions[image_id] = list()\n",
        "        # wrap description in tokens\n",
        "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "        # store\n",
        "        descriptions[image_id].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW_C1XnkL16J"
      },
      "source": [
        "## Tokenizer and Tokenizer helper functions\n",
        "Tokenizer encodes English captions to vectors,\n",
        "and transforms those vectors into uniform-length sequences. Tokenizer is fitted upon the training descriptions text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jsirLxko2cx"
      },
      "source": [
        "# Build tokenizer\n",
        "# Note: add limit to vocabulay?  ~9500 to 5000\n",
        "\n",
        "# convert a dictionary of clean descriptions (image_id: list of descriptions) to a general list of all descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        " \n",
        "#vocab_limit = 5000\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\t#tokenizer = Tokenizer(num_words=vocab_limit)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# Helper function; calculate the length of the description with the most words\n",
        "def calc_max_length(description):\n",
        "  lines = to_lines(description)\n",
        "  return max(len(d.split()) for d in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNsRF_pxMh-o"
      },
      "source": [
        "## Creating sequences\n",
        "\n",
        "Takes the tokenizer, the maximum length of the descriptions, dictionary of all descriptions, and dictionary of photo features and transforms data into input/output pairs of data for training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyLIMNJ_4OG"
      },
      "source": [
        "# create sequences of images, input sequences, and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos_features, vocab_size):\n",
        "  X1, X2, y = list(), list(), list()\n",
        "  # walk through each image id\n",
        "  for key, desc_list in descriptions.items():\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "      # encode the sequence\n",
        "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "      # split one sequence into multiple X,y pairs\n",
        "      for i in range(1, len(seq)):\n",
        "        # split into input and output pair - words up to i, and i (where i is the next word)\n",
        "        in_seq, out_seq, = seq[:i], seq[i]\n",
        "        # pad input sequence \n",
        "        in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n",
        "        # encode output sequence\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        # store\n",
        "        X1.append(photos_features[key][0])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "  return array(X1), array(X2), array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wihivupHoU0h"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, query, values): # hidden, enc_output\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        #return context_vector, attention_weights\n",
        "        return context_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvTp7sFZLpWH"
      },
      "source": [
        "## Bulding model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOjuI0zhwR-"
      },
      "source": [
        "# Defining the captioning model with Bahdanau Attention\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, query, values): # hidden, enc_output\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        #return context_vector, attention_weights\n",
        "        return context_vector\n",
        "\n",
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(2048,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\trnn = LSTM(256, return_sequences=True, return_state=True)\n",
        "\tenc_output, hidden_state = (rnn)(se2)\n",
        "\tattention = BahdanauAttention(2048)\n",
        "\tse3 = attention(hidden_state, enc_output)\n",
        "\t# decoder model\n",
        "\tdecoder1 = add([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summarize model\n",
        "\tprint(model.summary())\n",
        "\t#plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4zJlXBbA0jY"
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/4962\n",
        "\n",
        "from keras.layers import Flatten, Activation, RepeatVector, Permute\n",
        "\n",
        "# Defining the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(2048,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  activations = LSTM(256, return_sequences=True)(se2)\n",
        "  attention = Dense(1, activation='tanh')(activations)\n",
        "  attention = Flatten()(attention)\n",
        "  attention = Activation('softmax')(attention)\n",
        "  attention = RepeatVector(256)(attention)\n",
        "  attention = Permute([2, 1])(attention)\n",
        "  se3 = LSTM(256)(attention)\n",
        "  # decoder model\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDxdEJ9161w3",
        "outputId": "94b1740e-f7e8-4745-fdcc-9d7f12eafee7"
      },
      "source": [
        "model = define_model(vocab_size, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, 34, 256)      1940224     input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 34, 256)      0           embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  (None, 34, 256)      525312      dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 34, 1)        257         lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 34)           0           dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 34)           0           flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_4 (RepeatVector)  (None, 256, 34)      0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 2048)         0           input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, 34, 256)      0           repeat_vector_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 256)          524544      dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  (None, 256)          525312      permute_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 256)          0           dense_27[0][0]                   \n",
            "                                                                 lstm_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 256)          65792       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 7579)         1947803     dense_29[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 5,529,244\n",
            "Trainable params: 5,529,244\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaM85V-bpytE"
      },
      "source": [
        "## Load training data \n",
        "Creates input/output pairs for the training data <br>\n",
        "input: image features, text descriptions <br>\n",
        "output: next word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQ2Dpx7oLGV"
      },
      "source": [
        "# load training dataset\n",
        "filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', train)\n",
        "# photo features\n",
        "train_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', train)\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "\n",
        "# Embedding layer expects input_dim to be vocab size + 1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# determine the maximum sequence length\n",
        "max_length = calc_max_length(train_descriptions)\n",
        "# prepare sequences\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezIehr6yNu4Y"
      },
      "source": [
        "## Load validation data\n",
        "Creates input/output pairs for the validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL-h8sfDrYsr"
      },
      "source": [
        "# load validation set\n",
        "filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "validation = load_set(filename)\n",
        "# descriptions\n",
        "validation_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', validation)\n",
        "# photo features\n",
        "validation_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', validation)\n",
        "\n",
        "# prepare sequences\n",
        "X1val, X2val, yval = create_sequences(tokenizer, max_length, validation_descriptions, validation_features, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ4v2QJfN2nM"
      },
      "source": [
        "## Train model\n",
        "Models with improved loss are saved each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-7ckCgxmYwE",
        "outputId": "a618a7f2-1ac9-4dd5-8f05-f76919474cf9"
      },
      "source": [
        "# Fit model\n",
        "\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "\n",
        "# Adding checkpoint - save the model when it improves, \n",
        "# and then use the model with the best skill as the final model.\n",
        "# https://www.tensorflow.org/tutorials/keras/save_and_load\n",
        "# SavedModel_format rather than .h5, since saving a custom model\n",
        "\n",
        "# Define checkpoint callback\n",
        "filepath = '/content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# fit model\n",
        "model.fit([X1train, X2train], ytrain, epochs=5, verbose=1, callbacks=[checkpoint], validation_data=([X1val, X2val], yval))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 34, 256)      525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 34, 1)        257         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 34)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 34)           0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 256, 34)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 34, 256)      0           repeat_vector[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      permute[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,529,244\n",
            "Trainable params: 5,529,244\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "9576/9576 [==============================] - ETA: 0s - loss: 4.7468\n",
            "Epoch 00001: val_loss improved from inf to 4.36241, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep001-loss4.747-val_loss4.362\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep001-loss4.747-val_loss4.362/assets\n",
            "9576/9576 [==============================] - 841s 88ms/step - loss: 4.7468 - val_loss: 4.3624\n",
            "Epoch 2/5\n",
            "9576/9576 [==============================] - ETA: 0s - loss: 4.1646\n",
            "Epoch 00002: val_loss improved from 4.36241 to 4.21621, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep002-loss4.165-val_loss4.216\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep002-loss4.165-val_loss4.216/assets\n",
            "9576/9576 [==============================] - 852s 89ms/step - loss: 4.1646 - val_loss: 4.2162\n",
            "Epoch 3/5\n",
            "9576/9576 [==============================] - ETA: 0s - loss: 3.9812\n",
            "Epoch 00003: val_loss improved from 4.21621 to 4.17981, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep003-loss3.981-val_loss4.180\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep003-loss3.981-val_loss4.180/assets\n",
            "9576/9576 [==============================] - 857s 90ms/step - loss: 3.9812 - val_loss: 4.1798\n",
            "Epoch 4/5\n",
            "9576/9576 [==============================] - ETA: 0s - loss: 3.8838\n",
            "Epoch 00004: val_loss improved from 4.17981 to 4.14974, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep004-loss3.884-val_loss4.150\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep004-loss3.884-val_loss4.150/assets\n",
            "9576/9576 [==============================] - 843s 88ms/step - loss: 3.8838 - val_loss: 4.1497\n",
            "Epoch 5/5\n",
            "9576/9576 [==============================] - ETA: 0s - loss: 3.8259\n",
            "Epoch 00005: val_loss did not improve from 4.14974\n",
            "9576/9576 [==============================] - 824s 86ms/step - loss: 3.8259 - val_loss: 4.1577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feed11384a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZE436TEOEaB"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ene71bgK43GV"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# map an integer to word\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == integer:\n",
        "      return word\n",
        "  return none\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "  # seed generation process with start flag\n",
        "  in_text = 'startseq'\n",
        "  # iterate over the whole length of the sequence\n",
        "  for i in range(max_length):\n",
        "    # integer encode input sequence \n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    # pad input\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "    # predict next word\n",
        "    yhat = model.predict([photo, sequence], verbose=0)\n",
        "    # convert probability to an integer\n",
        "    yhat = argmax(yhat)\n",
        "    # map integer to word\n",
        "    word = word_for_id(yhat, tokenizer)\n",
        "    # stop if we cannot map the word\n",
        "    if word is None:\n",
        "      break\n",
        "    # append as input for generating the next word\n",
        "    in_text += ' ' + word\n",
        "    # stop if we predict the end of the sequence \n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  return in_text\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "  actual, predicted = list(), list()\n",
        "  # step over the whole set\n",
        "  for key, desc_list in descriptions.items():\n",
        "    # generate descriptions\n",
        "    yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "    # store actual and predicted\n",
        "    references = [d.split() for d in desc_list]\n",
        "    actual.append(references)\n",
        "    predicted.append(yhat.split())\n",
        "  # calculate BLEU scores\n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5FMEWu7qKfO"
      },
      "source": [
        "from keras.models import load_model\n",
        "from numpy import argmax\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = load(open('/content/drive/My Drive/ImageCaptioningProject/develop_tokenizer.pkl', 'rb'))\n",
        "# previously defined\n",
        "max_length = 34\n",
        "\n",
        "# load test set, unseen by the model\n",
        "filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', test)\n",
        "# photo features\n",
        "test_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', test)\n",
        "\n",
        "# load the model\n",
        "filename = '/content/drive/MyDrive/ImageCaptioningProject/NewModels/TransformerModelsWithLR/monday_model'\n",
        "saved_model = load_model(filename)\n",
        "\n",
        "# evaluate model\n",
        "evaluate_model(saved_model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJGoiTAIrsP"
      },
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "# extract features of new uncaptioned photos\n",
        "def extract_features(filename):\n",
        "  # load model\n",
        "  features_model = InceptionV3()\n",
        "  # omitting unneccessary classification layer\n",
        "  features_model = Model(inputs=features_model.inputs, outputs=features_model.layers[-2].output)\n",
        "  # load photo\n",
        "  image = load_img(filename, target_size=(299, 299))\n",
        "  # convert image pixels to numpy array\n",
        "  image = img_to_array(image)\n",
        "  # reshape image array for model input\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) \n",
        "  # prepare image for CNN. Normalizes image array to range [-1, 1], matching format of images used to train InceptionV3\n",
        "  image = preprocess_input(image)\n",
        "  # get image features\n",
        "  feature = features_model.predict(image, verbose=0)\n",
        "  return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6544tPRHO0Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7371498-74e1-4d92-dacd-37d2b4dcf763"
      },
      "source": [
        "from keras.models import load_model\n",
        "from numpy import argmax\n",
        "# BEST MODEL SO FAR: /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep004-loss3.884-val_loss4.150\n",
        "saved_model = load_model('/content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep004-loss3.884-val_loss4.150')\n",
        "\n",
        "photo = extract_features('/content/drive/MyDrive/ImageCaptioningProject/uncaptioned_images/example4.jpg')\n",
        "description = generate_desc(saved_model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq young boy in blue shirt is playing in the water endseq\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}