{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "II - ModelAndCaptioning - Adding Bahdanau Attention to Text with Glove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMXtZmkNaMoP"
      },
      "source": [
        "https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phtTRHEDQBqf",
        "outputId": "0d50843d-cc3d-468f-d5ed-a8043fd1e19a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zdpgg6tLiGr"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeGd3dQTMI1j"
      },
      "source": [
        "## File Loading Helper Functions\n",
        "Helps load previously processed image features and cleaned image descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzhn11C_UNDU"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load list of unique photo ids, derived from image file names\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "    # skip empty lines\n",
        "    if len(line) < 1:\n",
        "      continue\n",
        "    # get the image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    # load document\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "      # split line by white space\n",
        "      tokens = line.split()\n",
        "      # split id from description\n",
        "      image_id, image_desc = tokens[0], tokens[1:]\n",
        "      # skip images not in the set\n",
        "      if image_id in dataset:\n",
        "        # create list\n",
        "        if image_id not in descriptions:\n",
        "          descriptions[image_id] = list()\n",
        "        # wrap description in tokens\n",
        "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "        # store\n",
        "        descriptions[image_id].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW_C1XnkL16J"
      },
      "source": [
        "## Tokenizer and Tokenizer helper functions\n",
        "Tokenizer encodes English captions to vectors,\n",
        "and transforms those vectors into uniform-length sequences. Tokenizer is fitted upon the training descriptions text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jsirLxko2cx"
      },
      "source": [
        "# Build tokenizer\n",
        "# Note: add limit to vocabulay?  ~9500 to 5000\n",
        "\n",
        "# convert a dictionary of clean descriptions (image_id: list of descriptions) to a general list of all descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        " \n",
        "vocab_limit = 5000\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer(num_words=vocab_limit)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# Helper function; calculate the length of the description with the most words\n",
        "def calc_max_length(description):\n",
        "  lines = to_lines(description)\n",
        "  return max(len(d.split()) for d in lines)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZXkYxwdASX"
      },
      "source": [
        "## Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3xTV12dO9Sp"
      },
      "source": [
        "import numpy as np\n",
        "def get_dicts(descriptions, word_count_threshold=10):\n",
        "  all_train_captions = to_lines(train_descriptions)\n",
        "\n",
        "  # limited to words that are used at least 10 times\n",
        "  word_counts = {}\n",
        "  for sent in all_train_captions:\n",
        "      for w in sent.split(' '):\n",
        "          word_counts[w] = word_counts.get(w, 0) + 1\n",
        "  vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "  print('Vocabulary = %d' % (len(vocab)))\n",
        "\n",
        "  # Create two dictionaries: one to map word to an index, one to map index to a word\n",
        "  # Also adding 1 to length of vocabulary, appending 0s at end of all captions to make them equal length\n",
        "  ixtoword = {}\n",
        "  wordtoix = {}\n",
        "  ix = 1\n",
        "  for w in vocab:\n",
        "      wordtoix[w] = ix\n",
        "      ixtoword[ix] = w\n",
        "      ix += 1\n",
        "\n",
        "  vocab_size = len(ixtoword) + 1\n",
        "\n",
        "  return vocab_size, wordtoix, ixtoword\n",
        "\n",
        "\n",
        "def get_glove_embeddings(wordtoix, vocab_size):\n",
        "  embeddings_index = {}\n",
        "  f = open('/content/drive/MyDrive/ImageCaptioningProject/glove.6B.200d.txt', encoding='utf-8')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # make the matrix of shape ({vocab length},200) consisting of our vocabulary and the 200-d vector.\n",
        "  embedding_dim = 200\n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "  for word, i in wordtoix.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  \n",
        "  return embedding_matrix"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGkeNrsJc6kJ"
      },
      "source": [
        "# TEST - adding Glove embedding to our model \n",
        "# https://www.analyticsvidhya.com/blog/2020/11/create-your-own-image-caption-generator-using-keras/\n",
        "\n",
        "# Need: vocab size (~1651), description length (~34)\n",
        "\n",
        "all_train_captions = to_lines(train_descriptions)\n",
        "\n",
        "# limited to words that are used at least 10 times\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))\n",
        "\n",
        "# Create two dictionaries: one to map word to an index, one to map index to a word\n",
        "# Also adding 1 to length of vocabulary, appending 0s at end of all captions to make them equal length\n",
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1\n",
        "\n",
        "# Finding max length of caption\n",
        "all_desc = list()\n",
        "for key in train_descriptions.keys():\n",
        "    [all_desc.append(d) for d in train_descriptions[key]]\n",
        "lines = all_desc\n",
        "max_length = max(len(d.split()) for d in lines)\n",
        "\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t2h-XSsdELI",
        "outputId": "af1c7556-838e-40eb-ae45-c788941aac7f"
      },
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/MyDrive/ImageCaptioningProject/glove.6B.200d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# make the matrix of shape ({vocab length},200) consisting of our vocabulary and the 200-d vector.\n",
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in wordtoix.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNsRF_pxMh-o"
      },
      "source": [
        "## Creating sequences\n",
        "\n",
        "Takes the tokenizer, the maximum length of the descriptions, dictionary of all descriptions, and dictionary of photo features and transforms data into input/output pairs of data for training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyLIMNJ_4OG"
      },
      "source": [
        "# create sequences of images, input sequences, and output words for an image\n",
        "#def create_sequences(tokenizer, max_length, descriptions, photos_features, vocab_size):\n",
        "def create_sequences(wordtoix, max_length, descriptions, photos_features, vocab_size):\n",
        "  X1, X2, y = list(), list(), list()\n",
        "  # walk through each image id\n",
        "  for key, desc_list in descriptions.items():\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "      # encode the sequence\n",
        "      #seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "      seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "      # split one sequence into multiple X,y pairs\n",
        "      for i in range(1, len(seq)):\n",
        "        # split into input and output pair - words up to i, and i (where i is the next word)\n",
        "        in_seq, out_seq, = seq[:i], seq[i]\n",
        "        # pad input sequence \n",
        "        in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n",
        "        # encode output sequence\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        # store\n",
        "        X1.append(photos_features[key][0])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "  return array(X1), array(X2), array(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaM85V-bpytE"
      },
      "source": [
        "## Load training data \n",
        "Creates input/output pairs for the training data <br>\n",
        "input: image features, text descriptions <br>\n",
        "output: next word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQ2Dpx7oLGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0d680b-05db-46b1-ae62-317c67f5b2e8"
      },
      "source": [
        "# load training dataset\n",
        "filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', train)\n",
        "# photo features\n",
        "train_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', train)\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "\n",
        "# Embedding layer expects input_dim to be vocab size + 1\n",
        "#vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size, wordtoix, ixtoword = get_dicts(train_descriptions)\n",
        "glove_embeddings_matrix = get_glove_embeddings(wordtoix=wordtoix, vocab_size=vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = calc_max_length(train_descriptions)\n",
        "# prepare sequences\n",
        "X1train, X2train, ytrain = create_sequences(wordtoix, max_length, train_descriptions, train_features, vocab_size)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary = 1651\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezIehr6yNu4Y"
      },
      "source": [
        "## Load validation data\n",
        "Creates input/output pairs for the validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL-h8sfDrYsr"
      },
      "source": [
        "# load validation set\n",
        "filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "validation = load_set(filename)\n",
        "# descriptions\n",
        "validation_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', validation)\n",
        "# photo features\n",
        "validation_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', validation)\n",
        "\n",
        "# prepare sequences\n",
        "X1val, X2val, yval = create_sequences(wordtoix, max_length, validation_descriptions, validation_features, vocab_size)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvTp7sFZLpWH"
      },
      "source": [
        "## Bulding model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4zJlXBbA0jY"
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/4962\n",
        "\n",
        "from keras.layers import Flatten, Activation, RepeatVector, Permute\n",
        "\n",
        "# Defining the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(2048,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 200, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  activations = LSTM(256, return_sequences=True)(se2)\n",
        "  attention = Dense(1, activation='tanh')(activations)\n",
        "  attention = Flatten()(attention)\n",
        "  attention = Activation('softmax')(attention)\n",
        "  attention = RepeatVector(256)(attention)\n",
        "  attention = Permute([2, 1])(attention)\n",
        "  se3 = LSTM(256)(attention)\n",
        "  # decoder model\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "  # do not want to retrain the weights in text embedding layer (pre-trained Glove vectors)\n",
        "  model.layers[1].set_weights([glove_embeddings_matrix])\n",
        "  model.layers[1].trainable = False\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ4v2QJfN2nM"
      },
      "source": [
        "## Train model\n",
        "Models with improved loss are saved each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-7ckCgxmYwE",
        "outputId": "eaa8f489-e734-4e2f-9d4d-070e11cb977e"
      },
      "source": [
        "# Fit model\n",
        "\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "\n",
        "# Adding checkpoint - save the model when it improves, \n",
        "# and then use the model with the best skill as the final model.\n",
        "# https://www.tensorflow.org/tutorials/keras/save_and_load\n",
        "# SavedModel_format rather than .h5, since saving a custom model\n",
        "\n",
        "# Define checkpoint callback\n",
        "filepath = '/content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# fit model\n",
        "model.fit([X1train, X2train], ytrain, epochs=5, verbose=1, callbacks=[checkpoint], validation_data=([X1val, X2val], yval))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 34, 200)      330400      input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 34, 200)      0           embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 34, 256)      467968      dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 34, 1)        257         lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 34)           0           dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 34)           0           flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_5 (RepeatVector)  (None, 256, 34)      0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 2048)         0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "permute_5 (Permute)             (None, 34, 256)      0           repeat_vector_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 256)          524544      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 256)          525312      permute_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 256)          0           dense_20[0][0]                   \n",
            "                                                                 lstm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 256)          65792       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 1652)         424564      dense_22[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,338,837\n",
            "Trainable params: 2,008,437\n",
            "Non-trainable params: 330,400\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "9136/9136 [==============================] - ETA: 0s - loss: 4.3371\n",
            "Epoch 00001: val_loss improved from inf to 3.98032, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep001-loss4.337-val_loss3.980\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep001-loss4.337-val_loss3.980/assets\n",
            "9136/9136 [==============================] - 976s 107ms/step - loss: 4.3371 - val_loss: 3.9803\n",
            "Epoch 2/5\n",
            "9136/9136 [==============================] - ETA: 0s - loss: 3.8378\n",
            "Epoch 00002: val_loss improved from 3.98032 to 3.84285, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep002-loss3.838-val_loss3.843\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep002-loss3.838-val_loss3.843/assets\n",
            "9136/9136 [==============================] - 975s 107ms/step - loss: 3.8378 - val_loss: 3.8428\n",
            "Epoch 3/5\n",
            "9136/9136 [==============================] - ETA: 0s - loss: 3.7046\n",
            "Epoch 00003: val_loss improved from 3.84285 to 3.81387, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep003-loss3.705-val_loss3.814\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep003-loss3.705-val_loss3.814/assets\n",
            "9136/9136 [==============================] - 977s 107ms/step - loss: 3.7046 - val_loss: 3.8139\n",
            "Epoch 4/5\n",
            "9136/9136 [==============================] - ETA: 0s - loss: 3.6409\n",
            "Epoch 00004: val_loss improved from 3.81387 to 3.80195, saving model to /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep004-loss3.641-val_loss3.802\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep004-loss3.641-val_loss3.802/assets\n",
            "9136/9136 [==============================] - 973s 106ms/step - loss: 3.6409 - val_loss: 3.8019\n",
            "Epoch 5/5\n",
            "9136/9136 [==============================] - ETA: 0s - loss: 3.6069\n",
            "Epoch 00005: val_loss did not improve from 3.80195\n",
            "9136/9136 [==============================] - 959s 105ms/step - loss: 3.6069 - val_loss: 3.8067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fccbc4f0160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZE436TEOEaB"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ene71bgK43GV"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# map an integer to word\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == integer:\n",
        "      return word\n",
        "  return none\n",
        "\n",
        "# generate a description for an image\n",
        "#def generate_desc(model, tokenizer, photo, max_length):\n",
        "def generate_desc(model, wordtoix, ixtoword, photo, max_length):\n",
        "  # seed generation process with start flag\n",
        "  in_text = 'startseq'\n",
        "  # iterate over the whole length of the sequence\n",
        "  for i in range(max_length):\n",
        "    # integer encode input sequence \n",
        "    #sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "    # pad input\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "    # predict next word\n",
        "    yhat = model.predict([photo, sequence], verbose=0)\n",
        "    # convert probability to an integer\n",
        "    yhat = argmax(yhat)\n",
        "    # map integer to word\n",
        "    #word = word_for_id(yhat, tokenizer)\n",
        "    word = ixtoword[yhat]\n",
        "    # stop if we cannot map the word\n",
        "    if word is None:\n",
        "      break\n",
        "    # append as input for generating the next word\n",
        "    in_text += ' ' + word\n",
        "    # stop if we predict the end of the sequence \n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  return in_text\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "  actual, predicted = list(), list()\n",
        "  # step over the whole set\n",
        "  for key, desc_list in descriptions.items():\n",
        "    # generate descriptions\n",
        "    yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "    # store actual and predicted\n",
        "    references = [d.split() for d in desc_list]\n",
        "    actual.append(references)\n",
        "    predicted.append(yhat.split())\n",
        "  # calculate BLEU scores\n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5FMEWu7qKfO"
      },
      "source": [
        "from keras.models import load_model\n",
        "from numpy import argmax\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = load(open('/content/drive/My Drive/ImageCaptioningProject/develop_tokenizer.pkl', 'rb'))\n",
        "# previously defined\n",
        "max_length = 34\n",
        "\n",
        "# load test set, unseen by the model\n",
        "filename = '/content/drive/My Drive/ImageCaptioningProject/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('/content/drive/My Drive/ImageCaptioningProject/descriptions.txt', test)\n",
        "# photo features\n",
        "test_features = load_photo_features('/content/drive/My Drive/ImageCaptioningProject/features.pkl', test)\n",
        "\n",
        "# load the model\n",
        "filename = '/content/drive/MyDrive/ImageCaptioningProject/NewModels/TransformerModelsWithLR/monday_model'\n",
        "saved_model = load_model(filename)\n",
        "\n",
        "# evaluate model\n",
        "evaluate_model(saved_model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpJGoiTAIrsP"
      },
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "# extract features of new uncaptioned photos\n",
        "def extract_features(filename):\n",
        "  # load model\n",
        "  features_model = InceptionV3()\n",
        "  # omitting unneccessary classification layer\n",
        "  features_model = Model(inputs=features_model.inputs, outputs=features_model.layers[-2].output)\n",
        "  # load photo\n",
        "  image = load_img(filename, target_size=(299, 299))\n",
        "  # convert image pixels to numpy array\n",
        "  image = img_to_array(image)\n",
        "  # reshape image array for model input\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) \n",
        "  # prepare image for CNN. Normalizes image array to range [-1, 1], matching format of images used to train InceptionV3\n",
        "  image = preprocess_input(image)\n",
        "  # get image features\n",
        "  feature = features_model.predict(image, verbose=0)\n",
        "  return feature"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6544tPRHO0Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae228553-bd9f-45be-882a-fda7dab75f33"
      },
      "source": [
        "from keras.models import load_model\n",
        "from numpy import argmax\n",
        "# BEST MODEL SO FAR: /content/drive/MyDrive/ImageCaptioningProject/NewModels/TutorialModels/model-ep004-loss3.884-val_loss4.150\n",
        "saved_model = load_model('/content/drive/MyDrive/ImageCaptioningProject/NewModels/GloveModels/model-ep004-loss3.641-val_loss3.802')\n",
        "\n",
        "photo = extract_features('/content/drive/MyDrive/ImageCaptioningProject/uncaptioned_images/example3.jpg')\n",
        "description = generate_desc(saved_model, wordtoix, ixtoword, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq the boy is playing in the air endseq\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}